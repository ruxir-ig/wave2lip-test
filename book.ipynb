{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#@title (1) Setup\n",
    "\n",
    "\n",
    "# Check if required packages are installed, install if missing\n",
    "packages = [\n",
    "    ('yt-dlp', 'yt-dlp'),\n",
    "    ('face_alignment', 'face-alignment==1.3.5'),\n",
    "    ('imageio', 'imageio'),\n",
    "    ('scikit-image', 'scikit-image'),\n",
    "    ('scipy', 'scipy'),\n",
    "    ('torch', 'torch'),\n",
    "]\n",
    "\n",
    "for import_name, pip_name in packages:\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"✓ {import_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {pip_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('./.cache/torch/hub/checkpoints', exist_ok=True)\n",
    "\n",
    "# Clone first-order-model if not exists\n",
    "if not os.path.exists('./first-order-model'):\n",
    "    subprocess.run(['git', 'clone', '--depth', '1', \n",
    "                   'https://github.com/eyaler/first-order-model'], check=True)\n",
    "    print(\"✓ first-order-model cloned\")\n",
    "else:\n",
    "    print(\"✓ first-order-model already exists\")\n",
    "\n",
    "# Download model weights if not exists\n",
    "weights_to_download = [\n",
    "    ('vox-adv-cpk.pth.tar', 'https://openavatarify.s3.amazonaws.com/weights/vox-adv-cpk.pth.tar'),\n",
    "]\n",
    "\n",
    "for filename, url in weights_to_download:\n",
    "    filepath = f'./first-order-model/{filename}'\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        subprocess.run(['wget', '--no-check-certificate', '-nc', url, '-O', filepath], check=True)\n",
    "    else:\n",
    "        print(f\"✓ {filename} already exists\")\n",
    "\n",
    "print(\"\\n✓ Setup complete! Ready to proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb5fc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dailymotion] Extracting URL: https://dai.ly/k7ppSzt4SHe27PCGrY6\n",
      "[dailymotion] Downloading Access Token\n",
      "[dailymotion] k7ppSzt4SHe27PCGrY6: Downloading media JSON metadata\n",
      "[dailymotion] k7ppSzt4SHe27PCGrY6: Downloading metadata JSON\n",
      "[dailymotion] k7ppSzt4SHe27PCGrY6: Downloading m3u8 information\n",
      "[info] k7ppSzt4SHe27PCGrY6: Downloading 1 format(s): hls-720\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 3\n",
      "[download] Destination: ./video.mp4\n",
      "[download] 100% of    2.26MiB in 00:00:02 at 812.82KiB/s               \n",
      "[FixupM3u8] Fixing MPEG-TS in MP4 container of \"./video.mp4\"\n"
     ]
    }
   ],
   "source": [
    "#@title (2) Get the Driver video and Avatar image from the web\n",
    "#@markdown 1. You can change the URLs to your **own** stuff from most video platforms!\n",
    "#@markdown 2. Alternatively, you can upload **local** files in the next cells (2a, 2b)\n",
    "\n",
    "import subprocess\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "video_url = 'https://dai.ly/k7ppSzt4SHe27PCGrY6'\n",
    "limit_video_height = \"1080\"\n",
    "image_url = 'https://www.srugim.co.il/wp-content/uploads/2010/10/%D7%90%D7%97%D7%9E%D7%93-%D7%98%D7%99%D7%91%D7%99.jpg'\n",
    "\n",
    "if video_url:\n",
    "  if os.path.exists(\"./video.mp4\"):\n",
    "    os.remove(\"./video.mp4\")\n",
    "  if os.path.exists(\"./video\"):\n",
    "    os.remove(\"./video\")\n",
    "\n",
    "  subprocess.run([\n",
    "    \"yt-dlp\", \"--no-playlist\", \"-f\",\n",
    "    f\"bestvideo[ext=mp4][vcodec!*=av01][height<={limit_video_height}]+bestaudio[ext=m4a]/mp4[height<={limit_video_height}][vcodec!*=av01]/mp4[vcodec!*=av01]/mp4\",\n",
    "    video_url, \"--merge-output-format\", \"mp4\", \"-o\", \"./video.mp4\"\n",
    "  ])\n",
    "  \n",
    "  if os.path.exists(\"./video.mp4\"):\n",
    "    os.rename(\"./video.mp4\", \"./video\")\n",
    "\n",
    "if image_url:\n",
    "  urllib.request.urlretrieve(image_url, \"./image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17228ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (2a) Optionally upload local Driver video\n",
    "#@markdown Instructions: select a video file from your local system\n",
    "manually_upload_video = False #@param {type:\"boolean\"}\n",
    "if manually_upload_video:\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Open file dialog to select video\n",
    "    from tkinter import Tk, filedialog\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    video_path = filedialog.askopenfilename(\n",
    "        title=\"Select a video file\",\n",
    "        filetypes=[(\"Video files\", \"*.mp4 *.avi *.mov *.mkv\"), (\"All files\", \"*.*\")]\n",
    "    )\n",
    "    \n",
    "    if video_path:\n",
    "        shutil.copy(video_path, './video')\n",
    "        print(f\"Video uploaded from: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac68d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (2b) Optionally upload local Avatar image\n",
    "#@markdown Instructions: mark the checkbox + run the cell, and select your image file from a dialog\n",
    "manually_upload_image = False #@param {type:\"boolean\"}\n",
    "if manually_upload_image:\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Open file dialog to select image\n",
    "    from tkinter import Tk, filedialog\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    image_path = filedialog.askopenfilename(\n",
    "        title=\"Select an image file\",\n",
    "        filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp\"), (\"All files\", \"*.*\")]\n",
    "    )\n",
    "\n",
    "    if image_path:\n",
    "        shutil.copy(image_path, './image')\n",
    "        print(f\"Image uploaded from: {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0ce78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version n8.0.1 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with gcc 15.2.1 (GCC) 20251112\n",
      "  configuration: --prefix=/usr --disable-debug --disable-static --disable-stripping --enable-amf --enable-avisynth --enable-cuda-llvm --enable-lto --enable-fontconfig --enable-frei0r --enable-gmp --enable-gnutls --enable-gpl --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libdav1d --enable-libdrm --enable-libdvdnav --enable-libdvdread --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgsm --enable-libharfbuzz --enable-libiec61883 --enable-libjack --enable-libjxl --enable-libmodplug --enable-libmp3lame --enable-libopencore_amrnb --enable-libopencore_amrwb --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libplacebo --enable-libpulse --enable-librav1e --enable-librsvg --enable-librubberband --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtheora --enable-libv4l2 --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpl --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxcb --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-nvdec --enable-nvenc --enable-opencl --enable-opengl --enable-shared --enable-vapoursynth --enable-version3 --enable-vulkan\n",
      "  libavutil      60.  8.100 / 60.  8.100\n",
      "  libavcodec     62. 11.100 / 62. 11.100\n",
      "  libavformat    62.  3.100 / 62.  3.100\n",
      "  libavdevice    62.  1.100 / 62.  1.100\n",
      "  libavfilter    11.  4.100 / 11.  4.100\n",
      "  libswscale      9.  1.100 /  9.  1.100\n",
      "  libswresample   6.  1.100 /  6.  1.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'full_video':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf62.3.100\n",
      "  Duration: 00:00:09.68, start: 0.000000, bitrate: 1891 kb/s\n",
      "  Stream #0:0[0x1](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s, start 0.007007 (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1[0x2](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 736x720 [SAR 1:1 DAR 46:45], 1752 kb/s, 29.97 fps, 29.97 tbr, 90k tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (h264 (native) -> h264 (libx264))\n",
      "  Stream #0:0 -> #0:1 (aac (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x56426897b180] using SAR=1/1\n",
      "[libx264 @ 0x56426897b180] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x56426897b180] profile High, level 3.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x56426897b180] 264 - core 165 r3222 b35605a - H.264/MPEG-4 AVC codec - Copyleft 2003-2025 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=22 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'video':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf62.3.100\n",
      "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 736x720 [SAR 1:1 DAR 46:45], q=2-31, 29.97 fps, 30k tbn (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc62.11.100 libx264\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc62.11.100 aac\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video shortened and saved to video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[out#0/mp4 @ 0x5642689ba880] video:495KiB audio:150KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 1.973336%\n",
      "frame=  290 fps=0.0 q=-1.0 Lsize=     659KiB time=00:00:09.59 bitrate= 562.1kbits/s speed=14.8x elapsed=0:00:00.64    \n",
      "[libx264 @ 0x56426897b180] frame I:2     Avg QP:17.32  size: 19894\n",
      "[libx264 @ 0x56426897b180] frame P:74    Avg QP:20.64  size:  4247\n",
      "[libx264 @ 0x56426897b180] frame B:214   Avg QP:25.58  size:   713\n",
      "[libx264 @ 0x56426897b180] consecutive B-frames:  1.0%  1.4%  1.0% 96.6%\n",
      "[libx264 @ 0x56426897b180] mb I  I16..4: 24.3% 65.5% 10.2%\n",
      "[libx264 @ 0x56426897b180] mb P  I16..4:  2.1%  5.6%  0.2%  P16..4: 30.7%  7.4%  2.9%  0.0%  0.0%    skip:51.2%\n",
      "[libx264 @ 0x56426897b180] mb B  I16..4:  0.1%  0.2%  0.0%  B16..8: 23.2%  0.6%  0.1%  direct: 0.1%  skip:75.8%  L0:45.7% L1:53.2% BI: 1.1%\n",
      "[libx264 @ 0x56426897b180] 8x8 transform intra:69.3% inter:90.8%\n",
      "[libx264 @ 0x56426897b180] coded y,uvDC,uvAC intra: 51.6% 62.7% 11.9% inter: 3.1% 3.4% 0.0%\n",
      "[libx264 @ 0x56426897b180] i16 v,h,dc,p: 36% 30%  8% 26%\n",
      "[libx264 @ 0x56426897b180] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 21% 16%  5%  7%  7%  8%  7%  5%\n",
      "[libx264 @ 0x56426897b180] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31% 28% 11%  3%  8%  6%  6%  5%  2%\n",
      "[libx264 @ 0x56426897b180] i8c dc,h,v,p: 43% 18% 27% 12%\n",
      "[libx264 @ 0x56426897b180] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x56426897b180] ref P L0: 64.7%  9.7% 17.5%  8.1%\n",
      "[libx264 @ 0x56426897b180] ref B L0: 86.0% 11.5%  2.5%\n",
      "[libx264 @ 0x56426897b180] ref B L1: 94.7%  5.3%\n",
      "[libx264 @ 0x56426897b180] kb/s:418.77\n",
      "[aac @ 0x564268b90240] Qavg: 3281.983\n"
     ]
    }
   ],
   "source": [
    "#@title (3) Optionally (but recommended) shorten Driver video\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "start_seconds = 0 #@param {type:\"number\"}\n",
    "duration_seconds = 60 #@param {type:\"number\"}\n",
    "start_seconds = max(start_seconds, 0)\n",
    "duration_seconds = max(duration_seconds, 0)\n",
    "#@markdown (use duration_seconds = 0 for unrestricted duration)\n",
    "\n",
    "video_path = Path(\"./video\")\n",
    "full_video_path = Path(\"./full_video\")\n",
    "\n",
    "if start_seconds or duration_seconds:\n",
    "    # Move the video file to full_video to preserve original\n",
    "    if video_path.exists():\n",
    "        shutil.move(str(video_path), str(full_video_path))\n",
    "    else:\n",
    "        print(\"Video file not found at ./video\")\n",
    "        full_video_path = None\n",
    "\n",
    "    if full_video_path and full_video_path.exists():\n",
    "        # Prepare ffmpeg command\n",
    "        ffmpeg_cmd = [\n",
    "            \"ffmpeg\",\n",
    "            \"-ss\", str(start_seconds),\n",
    "            \"-t\", str(duration_seconds),\n",
    "            \"-i\", str(full_video_path),\n",
    "            \"-f\", \"mp4\",\n",
    "            str(video_path),\n",
    "            \"-y\"\n",
    "        ]\n",
    "        try:\n",
    "            subprocess.run(ffmpeg_cmd, check=True)\n",
    "            print(f\"Video shortened and saved to {video_path}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(\"Error occurred during video processing:\", e)\n",
    "    else:\n",
    "        print(\"Cannot process video as the source file does not exist.\")\n",
    "else:\n",
    "    print('Using full video.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8a64a41",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "unexpected EOF, expected 998062 more bytes. The file might be corrupted.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Use GPU if available, else fallback to CPU for FaceAlignment\u001b[39;00m\n\u001b[32m     65\u001b[39m device_type = \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m fa = \u001b[43mface_alignment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFaceAlignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_alignment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLandmarksType\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_2D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflip_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_bounding_box\u001b[39m(target_landmarks, expansion_factor=\u001b[32m1\u001b[39m):\n\u001b[32m     69\u001b[39m     target_landmarks = np.array(target_landmarks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/wave2lip/.venv/lib/python3.12/site-packages/face_alignment/api.py:69\u001b[39m, in \u001b[36mFaceAlignment.__init__\u001b[39m\u001b[34m(self, landmarks_type, network_size, device, flip_input, face_detector, verbose)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Get the face detector\u001b[39;00m\n\u001b[32m     67\u001b[39m face_detector_module = \u001b[38;5;28m__import__\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mface_alignment.detection.\u001b[39m\u001b[33m'\u001b[39m + face_detector,\n\u001b[32m     68\u001b[39m                                   \u001b[38;5;28mglobals\u001b[39m(), \u001b[38;5;28mlocals\u001b[39m(), [face_detector], \u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28mself\u001b[39m.face_detector = \u001b[43mface_detector_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFaceDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Initialise the face alignemnt networks\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28mself\u001b[39m.face_alignment_net = FAN(network_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/wave2lip/.venv/lib/python3.12/site-packages/face_alignment/detection/sfd/sfd_detector.py:22\u001b[39m, in \u001b[36mSFDDetector.__init__\u001b[39m\u001b[34m(self, device, path_to_detector, verbose)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Initialise the face detector\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path_to_detector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     model_weights = \u001b[43mload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels_urls\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms3fd\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     24\u001b[39m     model_weights = torch.load(path_to_detector)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/wave2lip/.venv/lib/python3.12/site-packages/torch/hub.py:880\u001b[39m, in \u001b[36mload_state_dict_from_url\u001b[39m\u001b[34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[39m\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[32m    879\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n\u001b[32m--> \u001b[39m\u001b[32m880\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/wave2lip/.venv/lib/python3.12/site-packages/torch/serialization.py:1554\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1552\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1553\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1554\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/wave2lip/.venv/lib/python3.12/site-packages/torch/serialization.py:1821\u001b[39m, in \u001b[36m_legacy_load\u001b[39m\u001b[34m(f, map_location, pickle_module, **pickle_load_args)\u001b[39m\n\u001b[32m   1819\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m deserialized_objects\n\u001b[32m   1820\u001b[39m typed_storage = deserialized_objects[key]\n\u001b[32m-> \u001b[39m\u001b[32m1821\u001b[39m \u001b[43mtyped_storage\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_untyped_storage\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_set_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1823\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf_should_read_directly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_element_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_storage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1826\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1827\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1828\u001b[39m     offset = f.tell()\n",
      "\u001b[31mRuntimeError\u001b[39m: unexpected EOF, expected 998062 more bytes. The file might be corrupted."
     ]
    }
   ],
   "source": [
    "# (4) Prepare assets\n",
    "# If you run out of RAM, it may mean your video is too large. \n",
    "# You can shorten it above (3), or try to use a video of smaller resolution.\n",
    "\n",
    "# ---- PARAMETERS ----\n",
    "# Set to True to attempt to center/crop to the face using face alignment.\n",
    "center_video_to_head = True\n",
    "crop_video_to_head = True\n",
    "video_crop_expansion_factor = 2.5\n",
    "center_image_to_head = True\n",
    "crop_image_to_head = False\n",
    "image_crop_expansion_factor = 2.5\n",
    "video_crop_expansion_factor = max(video_crop_expansion_factor, 1)\n",
    "image_crop_expansion_factor = max(image_crop_expansion_factor, 1)\n",
    "\n",
    "# ---- FILEPATHS ----\n",
    "# Change these filepaths to your local files as appropriate.\n",
    "# Example usage: place your files into the project folder as 'image.jpg' and 'video.mp4'\n",
    "SOURCE_IMAGE_PATH = 'image.jpg'\n",
    "DRIVING_VIDEO_PATH = 'video.mp4'\n",
    "OUT_VIDEO_PATH = 'input_concat.mp4'\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.transform import resize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import face_alignment\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Display utility for desktop: uses OpenCV if available, else prints path\n",
    "def show_video_file(video_path):\n",
    "    try:\n",
    "        import cv2\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Cannot open file {video_path}\")\n",
    "            return\n",
    "        print(\"Press 'q' in the video window to quit.\")\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            cv2.imshow('Output Video Preview', frame)\n",
    "            if cv2.waitKey(int(1000 // fps)) & 0xFF == ord('q'):\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    except ImportError:\n",
    "        print(f\"Video saved at '{video_path}' (Install OpenCV for preview window.)\")\n",
    "\n",
    "if not hasattr(face_alignment.utils, '_original_transform'):\n",
    "    face_alignment.utils._original_transform = face_alignment.utils.transform\n",
    "\n",
    "def patched_transform(point, center, scale, resolution, invert=False):\n",
    "    return face_alignment.utils._original_transform(\n",
    "        point, center, torch.tensor(scale, dtype=torch.float32), torch.tensor(resolution, dtype=torch.float32), invert)\n",
    "\n",
    "face_alignment.utils.transform = patched_transform\n",
    "\n",
    "# Use GPU if available, else fallback to CPU for FaceAlignment\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=True, device=device_type)\n",
    "\n",
    "def create_bounding_box(target_landmarks, expansion_factor=1):\n",
    "    target_landmarks = np.array(target_landmarks)\n",
    "    x_y_min = target_landmarks.reshape(-1, 68, 2).min(axis=1)\n",
    "    x_y_max = target_landmarks.reshape(-1, 68, 2).max(axis=1)\n",
    "    expansion_factor = (expansion_factor-1)/2\n",
    "    bb_expansion_x = (x_y_max[:, 0] - x_y_min[:, 0]) * expansion_factor\n",
    "    bb_expansion_y = (x_y_max[:, 1] - x_y_min[:, 1]) * expansion_factor\n",
    "    x_y_min[:, 0] -= bb_expansion_x\n",
    "    x_y_max[:, 0] += bb_expansion_x\n",
    "    x_y_min[:, 1] -= bb_expansion_y\n",
    "    x_y_max[:, 1] += bb_expansion_y\n",
    "    return np.hstack((x_y_min, x_y_max-x_y_min))\n",
    "\n",
    "def fix_dims(im):\n",
    "    if im.ndim == 2:\n",
    "        im = np.tile(im[..., None], [1, 1, 3])\n",
    "    return im[...,:3]\n",
    "\n",
    "def get_crop(im, center_face=True, crop_face=True, expansion_factor=1, landmarks=None):\n",
    "    im = fix_dims(im)\n",
    "    if (center_face or crop_face) and not landmarks:\n",
    "        landmarks = fa.get_landmarks_from_image(im)\n",
    "    if (center_face or crop_face) and landmarks:\n",
    "        rects = create_bounding_box(landmarks, expansion_factor=expansion_factor)\n",
    "        x0,y0,w,h = sorted(rects, key=lambda x: x[2]*x[3])[-1]\n",
    "        if crop_face:\n",
    "            s = max(h, w)\n",
    "            x0 += (w-s)//2\n",
    "            x1 = x0 + s\n",
    "            y0 += (h-s)//2\n",
    "            y1 = y0 + s\n",
    "        else:\n",
    "            img_h,img_w = im.shape[:2]\n",
    "            img_s = min(img_h,img_w)\n",
    "            x0 = min(max(0, x0+(w-img_s)//2), img_w-img_s)\n",
    "            x1 = x0 + img_s\n",
    "            y0 = min(max(0, y0+(h-img_s)//2), img_h-img_s)\n",
    "            y1 = y0 + img_s\n",
    "    else:\n",
    "        h,w = im.shape[:2]\n",
    "        s = min(h,w)\n",
    "        x0 = (w-s)//2\n",
    "        x1 = x0 + s\n",
    "        y0 = (h-s)//2\n",
    "        y1 = y0 + s\n",
    "    return int(x0),int(x1),int(y0),int(y1)\n",
    "\n",
    "def pad_crop_resize(im, x0=None, x1=None, y0=None, y1=None, new_h=256, new_w=256):\n",
    "    im = fix_dims(im)\n",
    "    h,w = im.shape[:2]\n",
    "    if x0 is None:\n",
    "      x0 = 0\n",
    "    if x1 is None:\n",
    "      x1 = w\n",
    "    if y0 is None:\n",
    "      y0 = 0\n",
    "    if y1 is None:\n",
    "      y1 = h\n",
    "    if x0<0 or x1>w or y0<0 or y1>h:\n",
    "        im = np.pad(im, pad_width=[(max(-y0,0),max(y1-h,0)),(max(-x0,0),max(x1-w,0)),(0,0)], mode='edge')\n",
    "    return resize(im[max(y0,0):y1-min(y0,0),max(x0,0):x1-min(x0,0)], (new_h, new_w))\n",
    "\n",
    "# Load source image\n",
    "if not os.path.exists(SOURCE_IMAGE_PATH):\n",
    "    print(f\"Source image file not found: {SOURCE_IMAGE_PATH}\")\n",
    "    sys.exit(1)\n",
    "source_image = imageio.imread(SOURCE_IMAGE_PATH)\n",
    "source_image = pad_crop_resize(source_image, *get_crop(source_image, center_face=center_image_to_head, crop_face=crop_image_to_head, expansion_factor=image_crop_expansion_factor))\n",
    "\n",
    "# Load driving video\n",
    "if not os.path.exists(DRIVING_VIDEO_PATH):\n",
    "    print(f\"Driving video file not found: {DRIVING_VIDEO_PATH}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "with imageio.get_reader(DRIVING_VIDEO_PATH, format='mp4') as reader:\n",
    "    fps = reader.get_meta_data()['fps']\n",
    "    driving_video = []\n",
    "    landmarks = None\n",
    "    i = 0\n",
    "    try:\n",
    "        for i, im in enumerate(reader):\n",
    "            if not crop_video_to_head:\n",
    "                break\n",
    "            landmarks = fa.get_landmarks_from_image(im)\n",
    "            if landmarks:\n",
    "                break\n",
    "        x0, x1, y0, y1 = get_crop(im, center_face=center_video_to_head, crop_face=crop_video_to_head, expansion_factor=video_crop_expansion_factor, landmarks=landmarks)\n",
    "        reader.set_image_index(0)\n",
    "        for im in reader:\n",
    "            driving_video.append(pad_crop_resize(im, x0, x1, y0, y1))\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "def vid_display(path, source, driving, generated=None):\n",
    "    assert len(driving) > 0\n",
    "    output_frames = [img_as_ubyte(np.hstack([source, driving[i]] + ([generated[i]] if generated else []))) for i in range(len(driving))]\n",
    "    imageio.mimwrite(path, output_frames, fps=fps)\n",
    "    print(f\"Output video saved at '{path}'.\")\n",
    "    show_video_file(path)\n",
    "\n",
    "if landmarks:\n",
    "    print(f'First found head in frame {i}')\n",
    "else:\n",
    "    print('No face landmarks found in driving video (first segment). You may want to check your video or settings.')\n",
    "\n",
    "vid_display(OUT_VIDEO_PATH, source_image, driving_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) Find best alignment\n",
    "\n",
    "import os\n",
    "\n",
    "# Set working directory to local first-order-model directory if needed\n",
    "first_order_model_path = os.path.abspath(\"first-order-model\")\n",
    "if os.path.exists(first_order_model_path):\n",
    "    os.chdir(first_order_model_path)\n",
    "else:\n",
    "    raise FileNotFoundError(\"first-order-model folder not found in the current directory.\")\n",
    "\n",
    "from demo import load_checkpoints\n",
    "generator, kp_detector = load_checkpoints(\n",
    "    config_path=os.path.join(first_order_model_path, 'config/vox-adv-256.yaml'),\n",
    "    checkpoint_path=os.path.join(first_order_model_path, 'vox-adv-cpk.pth.tar')\n",
    ")\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "def normalize_kp(kps):\n",
    "    max_area = 0\n",
    "    max_kp = None\n",
    "    for kp in kps:\n",
    "        kp = kp - kp.mean(axis=0, keepdims=True)\n",
    "        area = ConvexHull(kp[:, :2]).volume\n",
    "        area = np.sqrt(area)\n",
    "        kp[:, :2] = kp[:, :2] / area\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            max_kp = kp\n",
    "    return max_kp\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "kp_source = fa.get_landmarks_from_image(255 * source_image)\n",
    "if kp_source:\n",
    "    norm_kp_source = normalize_kp(kp_source)\n",
    "\n",
    "norm = float('inf')\n",
    "best = 0\n",
    "best_kp_driving = None\n",
    "for i, image in tqdm(enumerate(driving_video)):\n",
    "    kp_driving = fa.get_landmarks_from_image(255 * image)\n",
    "    if kp_driving:\n",
    "        norm_kp_driving = normalize_kp(kp_driving)\n",
    "        if kp_source:\n",
    "            new_norm = (np.abs(norm_kp_source - norm_kp_driving) ** 2).sum()\n",
    "            if new_norm < norm:\n",
    "                norm = new_norm\n",
    "                best = i\n",
    "                best_kp_driving = kp_driving\n",
    "        else:\n",
    "            best_kp_driving = kp_driving\n",
    "            break\n",
    "\n",
    "from logger import Visualizer\n",
    "\n",
    "vis = Visualizer(kp_size=3, colormap='gist_rainbow')\n",
    "source_with_kp = vis.draw_image_with_kp(\n",
    "    source_image,\n",
    "    kp_source[0]*2/np.array(source_image.shape[:2][::-1])[np.newaxis] - 1\n",
    ") if kp_source else source_image\n",
    "driving_with_kp = vis.draw_image_with_kp(\n",
    "    driving_video[best],\n",
    "    best_kp_driving[0]*2/np.array(driving_video[best].shape[:2][::-1])[np.newaxis] - 1\n",
    ") if best_kp_driving else driving_video[best]\n",
    "\n",
    "print('\\nbest frame=%d' % best)\n",
    "\n",
    "# Visualize using OpenCV (cv2.imshow), which works locally\n",
    "import cv2\n",
    "\n",
    "show_img = (np.hstack([source_with_kp, driving_with_kp])[..., ::-1] * 255).astype(np.uint8)\n",
    "cv2.imshow(\"Source with Keypoints (left) / Driving with Keypoints (right)\", show_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded3923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) Animate\n",
    "\n",
    "exaggerate_factor = 1  # You can adjust this between 0.1 and 5\n",
    "adapt_movement_scale = True\n",
    "use_relative_movement = True\n",
    "use_relative_jacobian = True\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import imageio\n",
    "from skimage.img_as_ubyte import img_as_ubyte\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def full_normalize_kp(kp_source, kp_driving, kp_driving_initial, adapt_movement_scale=False,\n",
    "                      use_relative_movement=False, use_relative_jacobian=False, exaggerate_factor=1):\n",
    "    if adapt_movement_scale:\n",
    "        source_area = ConvexHull(kp_source['value'][0].data.cpu().numpy()).volume\n",
    "        driving_area = ConvexHull(kp_driving_initial['value'][0].data.cpu().numpy()).volume\n",
    "        adapt_movement_scale = np.sqrt(source_area) / np.sqrt(driving_area)\n",
    "    else:\n",
    "        adapt_movement_scale = 1\n",
    "\n",
    "    kp_new = {k: v for k, v in kp_driving.items()}\n",
    "\n",
    "    if use_relative_movement:\n",
    "        kp_value_diff = (kp_driving['value'] - kp_driving_initial['value'])\n",
    "        kp_value_diff *= adapt_movement_scale * exaggerate_factor\n",
    "        kp_new['value'] = kp_value_diff + kp_source['value']\n",
    "\n",
    "        if use_relative_jacobian:\n",
    "            jacobian_diff = torch.matmul(kp_driving['jacobian'], torch.inverse(kp_driving_initial['jacobian']))\n",
    "            kp_new['jacobian'] = torch.matmul(jacobian_diff, kp_source['jacobian'])\n",
    "\n",
    "    return kp_new\n",
    "\n",
    "def make_animation(source_image, driving_video, generator, kp_detector, adapt_movement_scale=False,\n",
    "                   use_relative_movement=False, use_relative_jacobian=False, cpu=False, exaggerate_factor=1):\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            source = source.cuda()\n",
    "        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "        kp_source = kp_detector(source)\n",
    "        kp_driving_initial = kp_detector(driving[:, :, 0])\n",
    "\n",
    "        for frame_idx in tqdm(range(driving.shape[2])):\n",
    "            driving_frame = driving[:, :, frame_idx]\n",
    "            if not cpu:\n",
    "                driving_frame = driving_frame.cuda()\n",
    "            kp_driving = kp_detector(driving_frame)\n",
    "            kp_norm = full_normalize_kp(\n",
    "                kp_source=kp_source, kp_driving=kp_driving,\n",
    "                kp_driving_initial=kp_driving_initial, adapt_movement_scale=adapt_movement_scale,\n",
    "                use_relative_movement=use_relative_movement, use_relative_jacobian=use_relative_jacobian,\n",
    "                exaggerate_factor=exaggerate_factor\n",
    "            )\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "    return predictions\n",
    "\n",
    "# Set local working directory for outputs\n",
    "output_dir = \"./output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Animations\n",
    "predictions_forward = make_animation(\n",
    "    source_image, driving_video[best:], generator, kp_detector, \n",
    "    adapt_movement_scale=adapt_movement_scale, \n",
    "    use_relative_movement=use_relative_movement,\n",
    "    use_relative_jacobian=use_relative_jacobian, exaggerate_factor=exaggerate_factor\n",
    ")\n",
    "predictions_backward = make_animation(\n",
    "    source_image, driving_video[:(best+1)][::-1], generator, kp_detector, \n",
    "    adapt_movement_scale=adapt_movement_scale, \n",
    "    use_relative_movement=use_relative_movement,\n",
    "    use_relative_jacobian=use_relative_jacobian, exaggerate_factor=exaggerate_factor\n",
    ")\n",
    "\n",
    "# Write video\n",
    "full_frames = predictions_backward[::-1] + predictions_forward[1:]\n",
    "out_mp4 = os.path.join(output_dir, \"generated.mp4\")\n",
    "imageio.mimwrite(out_mp4, [img_as_ubyte(frame) for frame in full_frames], fps=fps)\n",
    "\n",
    "# If you want to mix audio and/or re-encode:\n",
    "import subprocess\n",
    "\n",
    "input_video_path = out_mp4  # Generated video\n",
    "original_audio_path = \"video.mp4\"  # You should provide this path, usually your input video\n",
    "final_output = os.path.join(output_dir, \"final.mp4\")\n",
    "\n",
    "# You must set original_audio_path to the correct path of the original video that has audio\n",
    "ffmpeg_cmd = [\n",
    "    \"ffmpeg\",\n",
    "    \"-i\", input_video_path,\n",
    "    \"-i\", original_audio_path,\n",
    "    \"-c:v\", \"libx264\",\n",
    "    \"-c:a\", \"aac\",\n",
    "    \"-map\", \"0:v\",\n",
    "    \"-map\", \"1:a?\",\n",
    "    \"-pix_fmt\", \"yuv420p\",\n",
    "    final_output,\n",
    "    \"-y\"\n",
    "]\n",
    "subprocess.run(ffmpeg_cmd, check=True)\n",
    "\n",
    "print(f\"Video saved at {final_output}\")\n",
    "\n",
    "# Video display (OpenCV window)\n",
    "import cv2\n",
    "\n",
    "for frame in full_frames:\n",
    "    frame_bgr = (frame[..., ::-1] * 255).astype(np.uint8)\n",
    "    cv2.imshow(\"Animation\", frame_bgr)\n",
    "    if cv2.waitKey(int(1000//fps)) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) Save and Notify\n",
    "# This cell informs the user where the generated video is saved locally.\n",
    "\n",
    "output_path = os.path.abspath(final_output) if 'final_output' in locals() else os.path.abspath('final.mp4')\n",
    "print(f\"\\nYour video is saved at: {output_path}\")\n",
    "print(\"Please open this file using your preferred video player.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605bb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8) Optionally apply Wav2Lip post processing (local version, not Colab)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "wav2lip_post_processing = True  # Set to False to skip post-processing\n",
    "smooth_face_detection = True    # Set to False if you don't want smoothing\n",
    "\n",
    "if wav2lip_post_processing:\n",
    "    # Install required packages if missing\n",
    "    try:\n",
    "        import librosa\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"librosa==0.9.2\"])\n",
    "    try:\n",
    "        import gdown\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gdown\"])\n",
    "\n",
    "    # Set up paths\n",
    "    work_dir = os.path.abspath(\"./Wav2Lip_work\")\n",
    "    wav2lip_dir = os.path.join(work_dir, \"Wav2Lip\")\n",
    "    checkpoints_dir = os.path.join(wav2lip_dir, \"checkpoints\")\n",
    "    fd_sfd_dir = os.path.join(wav2lip_dir, \"face_detection\", \"detection\", \"sfd\")\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "    os.makedirs(fd_sfd_dir, exist_ok=True)\n",
    "\n",
    "    final_mp4_src = os.path.abspath(\"final.mp4\")\n",
    "    final_wav2lip_out = os.path.join(work_dir, \"final_wav2lip.mp4\")\n",
    "    faulty_frame_jpg = os.path.join(wav2lip_dir, \"temp\", \"faulty_frame.jpg\")\n",
    "\n",
    "    # Remove previous output if it exists\n",
    "    if os.path.exists(final_wav2lip_out):\n",
    "        os.remove(final_wav2lip_out)\n",
    "\n",
    "    # Clone Wav2Lip repo if needed\n",
    "    if not os.path.exists(wav2lip_dir):\n",
    "        subprocess.check_call([\n",
    "            \"git\", \"clone\", \"--depth\", \"1\",\n",
    "            \"https://github.com/eyaler/Wav2Lip.git\",\n",
    "            wav2lip_dir\n",
    "        ])\n",
    "\n",
    "    # Download wav2lip_gan.pth if needed\n",
    "    wav2lip_gan_path = os.path.join(checkpoints_dir, \"wav2lip_gan.pth\")\n",
    "    if not os.path.exists(wav2lip_gan_path):\n",
    "        # Try google drive (gdown) first\n",
    "        import gdown\n",
    "        gdown.download(\n",
    "            'https://drive.google.com/uc?id=1dwHujX7RVNCvdR1RR93z0FS2T2yzqup9',\n",
    "            wav2lip_gan_path, quiet=False)\n",
    "        # If download didn't succeed, try fallback hosting\n",
    "        if not os.path.exists(wav2lip_gan_path):\n",
    "            import urllib.request\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://eyalgruss.com/fomm/wav2lip_gan.pth\", wav2lip_gan_path)\n",
    "\n",
    "    # Download s3fd face detector if needed\n",
    "    s3fd_path = os.path.join(fd_sfd_dir, \"s3fd.pth\")\n",
    "    if not os.path.exists(s3fd_path):\n",
    "        import urllib.request\n",
    "        s3fd_url = \"https://github.com/clcarwin/sfd_pytorch/releases/download/v1.0/s3fd-619a316812.pth\"\n",
    "        urllib.request.urlretrieve(s3fd_url, s3fd_path)\n",
    "\n",
    "    # Clear Wav2Lip/temp if exists\n",
    "    temp_dir = os.path.join(wav2lip_dir, \"temp\")\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "\n",
    "    # Build inference command\n",
    "    nosmooth = [] if smooth_face_detection else [\"--nosmooth\"]\n",
    "    inference_py = os.path.join(wav2lip_dir, \"inference.py\")\n",
    "    args = [\n",
    "        sys.executable, inference_py,\n",
    "        \"--checkpoint_path\", wav2lip_gan_path,\n",
    "        \"--face\", final_mp4_src,\n",
    "        \"--audio\", final_mp4_src,\n",
    "        \"--pads\", \"0\", \"20\", \"0\", \"0\",\n",
    "        \"--outfile\", final_wav2lip_out\n",
    "    ] + nosmooth\n",
    "\n",
    "    # Run inference\n",
    "    subprocess.check_call(args)\n",
    "\n",
    "    # If faulty_frame.jpg exists, try fallback box inference\n",
    "    if os.path.exists(faulty_frame_jpg):\n",
    "        import cv2\n",
    "        print('\\nFace not detected - will use whole frame')\n",
    "        video_stream = cv2.VideoCapture(final_mp4_src)\n",
    "        still_reading, frame = video_stream.read()\n",
    "        if not still_reading:\n",
    "            raise RuntimeError(\"Can't read the frame from final.mp4 for bounding box calculation.\")\n",
    "        x1 = y1 = 0\n",
    "        y2, x2 = frame.shape[:2]\n",
    "        # assuming h is intended as height\n",
    "        h = y2\n",
    "        if x2 > h:\n",
    "            x1 = (x2 - h) // 2\n",
    "            x2 = x1 + y2\n",
    "        args_box = [\n",
    "            sys.executable, inference_py,\n",
    "            \"--checkpoint_path\", wav2lip_gan_path,\n",
    "            final_mp4_src,  # face\n",
    "            \"--audio\", final_mp4_src,\n",
    "            \"--box\", str(y1), str(y2), str(x1), str(x2),\n",
    "            \"--pads\", \"0\", \"20\", \"0\", \"0\",\n",
    "            \"--outfile\", final_wav2lip_out\n",
    "        ]\n",
    "        subprocess.check_call(args_box)\n",
    "\n",
    "    print(f\"Wav2Lip post-processing complete. Output: {final_wav2lip_out}\")\n",
    "    print(\"You can now open the resulting video file locally.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
